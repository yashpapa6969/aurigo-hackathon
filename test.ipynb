{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VEMKATESH R\\Desktop\\aurigo\\aurigo-hackathon\\get_material.py:40: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m load_dotenv()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_scores_for_multiple_projects\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mget_material\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_materials\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_n_days_forecast, get_num_workers\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_scores_for_multiple_projects\n",
      "File \u001b[1;32mc:\\Users\\VEMKATESH R\\Desktop\\aurigo\\aurigo-hackathon\\get_material.py:76\u001b[0m\n\u001b[0;32m     48\u001b[0m     data: \u001b[38;5;28mlist\u001b[39m[MaterialBase]\n\u001b[0;32m     50\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate(\n\u001b[0;32m     51\u001b[0m     [\n\u001b[0;32m     52\u001b[0m          (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an expert construction planner with access to a vast knowledge base and real-time web search capabilities. The response must be in below JSON format strictly without any explanation and comments:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m     ]\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m llm_with_tools \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm_with_tools\n\u001b[0;32m     78\u001b[0m parser \u001b[38;5;241m=\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mMaterialResponse)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:1119\u001b[0m, in \u001b[0;36mBaseChatModel.bind_tools\u001b[1;34m(self, tools, **kwargs)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_tools\u001b[39m(\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1114\u001b[0m     tools: Sequence[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1118\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[LanguageModelInput, BaseMessage]:\n\u001b[1;32m-> 1119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "from cal import compute_scores_for_multiple_projects\n",
    "from get_material import get_materials\n",
    "from ml_models.ml_pipeline import get_n_days_forecast, get_num_workers\n",
    "\n",
    "from cal import compute_scores_for_multiple_projects\n",
    "\n",
    "\n",
    "def pipeline(projects_example, dynamic_weights, OPENAI_API_KEY= os.getenv('OPENAI_API_KEY')):\n",
    "    res = compute_scores_for_multiple_projects(\n",
    "        projects_example,\n",
    "        dynamic_weights,\n",
    "        OPENAI_API_KEY\n",
    "    )\n",
    "    print(res)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "projects_example = [\n",
    "    {\n",
    "        \"projectId\": \"101\",\n",
    "        \"tasks\": [\"Build elevated track\", \"Install stations\", \"Lay electrical lines\"],\n",
    "        \"timeline\": 18,\n",
    "        \"budget\": 500,\n",
    "        \"location\": \"Urban Metro Region\"\n",
    "    },\n",
    "    {\n",
    "        \"projectId\": \"202\",\n",
    "        \"tasks\": [\n",
    "            \"Clear and grade existing roadway\",\n",
    "            \"Lay asphalt and expand lanes\",\n",
    "            \"Install drainage and stormwater systems\"\n",
    "        ],\n",
    "        \"timeline\": 12,\n",
    "        \"budget\": 450,\n",
    "        \"location\": \"Suburban / Semi-urban\"\n",
    "    },\n",
    "    {\n",
    "        \"projectId\": \"203\",\n",
    "        \"tasks\": [\n",
    "            \"Conduct geotechnical surveys and soil testing\",\n",
    "            \"Build foundation piles and piers\",\n",
    "            \"Assemble steel beams and girders\"\n",
    "        ],\n",
    "        \"timeline\": 18,\n",
    "        \"budget\": 750,\n",
    "        \"location\": \"Riverine / Rural Region\"\n",
    "    }\n",
    "]\n",
    "\n",
    "dynamic_weights = {\n",
    "    \"w_cost_benefit\": 0.25,\n",
    "    \"w_socio_impact\": 0.30,\n",
    "    \"w_risk_resilience\": 0.20,\n",
    "    \"w_strategic_align\": 0.25\n",
    "}\n",
    "\n",
    "pipeline(projects_example, dynamic_weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 76\u001b[0m\n\u001b[0;32m     48\u001b[0m     data: \u001b[38;5;28mlist\u001b[39m[MaterialBase]\n\u001b[0;32m     50\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate(\n\u001b[0;32m     51\u001b[0m     [\n\u001b[0;32m     52\u001b[0m          (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an expert construction planner with access to a vast knowledge base and real-time web search capabilities. The response must be in below JSON format strictly without any explanation and comments:\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m     ]\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 76\u001b[0m llm_with_tools \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm_with_tools\n\u001b[0;32m     78\u001b[0m parser \u001b[38;5;241m=\u001b[39m PydanticOutputParser(pydantic_object\u001b[38;5;241m=\u001b[39mMaterialResponse)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\chat_models.py:1119\u001b[0m, in \u001b[0;36mBaseChatModel.bind_tools\u001b[1;34m(self, tools, **kwargs)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_tools\u001b[39m(\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1114\u001b[0m     tools: Sequence[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1118\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable[LanguageModelInput, BaseMessage]:\n\u001b[1;32m-> 1119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig, chain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import tavily\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key:\\n\")\n",
    "    \n",
    "\n",
    "tool = TavilySearchResults(\n",
    "    max_results=5,\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True,\n",
    ")\n",
    "\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "class MaterialBase(BaseModel):\n",
    "    material: str\n",
    "    price: int\n",
    "\n",
    "class MaterialResponse(BaseModel):\n",
    "    data: list[MaterialBase]\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "         (\"system\", \"\"\"You are an expert construction planner with access to a vast knowledge base and real-time web search capabilities. The response must be in below JSON format strictly without any explanation and comments:\n",
    "        {{\n",
    "            \"data\": [\n",
    "                {{\n",
    "                    \"material\": \"Material 1\", \n",
    "                    \"price\": 1000\n",
    "                }},\n",
    "                {{\n",
    "                    \"material\": \"Material 2\", \n",
    "                    \"price\": 1500\n",
    "                }},\n",
    "                {{\n",
    "                    \"material\": \"Material 3\", \n",
    "                    \"price\": 2000\n",
    "                }}\n",
    "                // Add more materials and their prices as needed\n",
    "            ]\n",
    "        }}\"\"\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools([tool])\n",
    "llm_chain = prompt | llm_with_tools\n",
    "parser = PydanticOutputParser(pydantic_object=MaterialResponse)\n",
    "\n",
    "@chain\n",
    "def tool_chain(user_input: str, config: RunnableConfig):\n",
    "    input_ = {\"user_input\": user_input}\n",
    "    ai_msg = llm_chain.invoke(input_, config=config)\n",
    "    tool_msgs = tool.batch(ai_msg.tool_calls, config=config)\n",
    "    response = llm_chain.invoke({**input_, \"messages\": [ai_msg, *tool_msgs]}, config=config)\n",
    "    return response\n",
    "\n",
    "    \n",
    "\n",
    "def get_materials(user_input:str):\n",
    "    response = tool_chain.invoke(user_input)\n",
    "    parsed_response = parser.parse(response.content)\n",
    "    return parsed_response.model_dump()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
